# -*- coding: utf-8 -*-
"""Bert_sklearn.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1E1d9BbxRyMJmOs9vAgmPRRAFaiPEb2gV
"""

!git clone -b master https://github.com/charles9n/bert-sklearn
!pip install bert-sklearn/.

! pip3 install vncorenlp
! wget 'https://github.com/vncorenlp/VnCoreNLP/archive/v1.1.1.zip' -O ./models.$$ && unzip -o ./models.$$ && rm -r ./models.$$

import os
import math
import random
import csv
import sys

import numpy as np
import pandas as pd#
from sklearn import metrics
from sklearn.metrics import f1_score
from sklearn.metrics import classification_report
from sklearn.model_selection import train_test_split#
import statistics as stats

from bert_sklearn import BertClassifier
from bert_sklearn import BertRegressor
from bert_sklearn import BertTokenClassifier
from bert_sklearn import load_model

from vncorenlp import VnCoreNLP#

VnCoreNLP_path = "VnCoreNLP-1.1.1/VnCoreNLP-1.1.1.jar"
rdrsegmenter  = VnCoreNLP(VnCoreNLP_path,port = 9000, annotators='wseg',quiet=False)

txt = 'ai mù công nghệ mới mua thôi'
print(rdrsegmenter .tokenize(txt))

train = pd.read_csv('/content/drive/My Drive/project_devC/train.csv')
train.head()

test = pd.read_csv('/content/drive/My Drive/project_devC/test.csv')
test.head()

print(test['comment'].isnull().value_counts())
test['comment'] = test['comment'].fillna(" ")
print(test['comment'].isnull().value_counts())

def text_normalize(df, tokenize=False):
    # Convert text to lowercase
    df['comment'] = df['comment'].str.lower()
    # Remove numbers and words with numbers
    df['comment'] = df['comment'].str.replace('\w*\d\w*', ' ')
    # Remove punctuation
    df['comment'] = df['comment'].str.replace('[^\w\s]', ' ')
    # Remove whitespaces
    df['comment'] =  df['comment'].str.split().apply(lambda x : ' '.join(word for word in x))
    # Tokenize
    if tokenize:
        text = df['comment'].apply(lambda x : rdrsegmenter.tokenize(x))

text_normalize(train)
text_normalize(test)
train.head()

train_text = train['comment']
train_label = train['label']

train_sents, val_sents, train_label, val_labels = train_test_split(train_text, train_label, test_size=0.2)

train_sents.head()

model = BertClassifier(max_seq_length=128,
                       train_batch_size=32,
                       epochs=5,
                       bert_model='bert-base-multilingual-cased')
model

# Commented out IPython magic to ensure Python compatibility.
# %%time
# history = model.fit(train_text, train_label)

accy = model.score(val_sents, val_labels)

# make class probability predictions
y_prob = model.predict_proba(val_sents)
print("class prob estimates:\n", y_prob)

# make predictions
y_pred = model.predict(val_sents)
print("Accuracy: %0.2f%%"%(metrics.accuracy_score(y_pred, val_labels) * 100))

target_names = ['negative', 'positive']
print(classification_report(val_labels, y_pred, target_names=target_names))

X_test = test['comment']
test_id = test['id']

y_pred = model.predict(X_test)

print(y_pred)

res = pd.DataFrame({'id' : test_id,
       'label': y_pred})
res.to_csv('sample_submission.csv',index=False)