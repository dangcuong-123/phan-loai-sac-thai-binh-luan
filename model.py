# -*- coding: utf-8 -*-
"""Model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PcagnjlSNcNXt90MZO1U5KxDW2JhtYkn
"""

from google.colab import drive
drive.mount('/content/drive')

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

df_train = pd.read_csv("big_train.csv")
df_train

df_train.dropna(inplace=True)
df_train.isnull().sum()

y = df_train.label
y

import re 

def preprocessor(text):
    """ Return a cleaned version of text
    """
    # Remove HTML markup
    text = re.sub('<[^>]*>', '', text)
    # Xóa dấu chấm, phẩy, hỏi ở cuối câu
    text = re.sub(r"[\.,\?]+$-", "", text)
    # Save emoticons for later appending
    emoticons = re.findall('(?::|;|=)(?:-)?(?:\)|\(|D|P)', text)
    
    # Xóa tất cả dấu chấm, phẩy, chấm phẩy, chấm thang, ... trong câu
    text = text.replace(",", " ").replace(".", " ") \
        .replace(";", " ").replace("“", " ") \
        .replace(":", " ").replace("”", " ") \
        .replace('"', " ").replace("'", " ") \
        .replace("!", " ").replace("?", " ") \
        .replace("-", " ").replace("?", " ")
 
    text = text.strip()
    # Remove any non-word character and append the emoticons,
    # removing the nose character for standarization. Convert to lower case
    text = (re.sub('[\W]+', ' ', text.lower()) + ' ' + ' '.join(emoticons).replace('-', ''))
    
    return text

df_train.content = df_train.content.apply(lambda x : preprocessor(x))

from pyvi import ViTokenizer, ViPosTagger

def tokenizer(text):
    return ViTokenizer.tokenize(text)

df_train.content = df_train.content.apply(lambda x : tokenizer(x))

df_train.head()

from sklearn.feature_extraction.text import TfidfVectorizer

vectorizer = TfidfVectorizer(ngram_range = (1,2))
X = vectorizer.fit_transform(df_train['content'].values)

idfs = vectorizer.idf_
dict_idf = dict(zip(vectorizer.get_feature_names(), idfs))
idfs
print(len(dict_idf))
stop_word_list = []
for word, idf in dict_idf.items():
    if idf >= 9:
        stop_word_list.append(word)
print(len(stop_word_list))

vectorizer = TfidfVectorizer(stop_words = stop_word_list, ngram_range = (1,3))
X = vectorizer.fit_transform(df_train.content.values)

from sklearn.metrics import confusion_matrix, accuracy_score, classification_report
from sklearn.metrics import log_loss

from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test = train_test_split(X,y, test_size=0.15)
print(X_train.shape, X_test.shape)

from sklearn import svm
model = svm.SVC(C = 5,probability = True)
model.fit(X_train, y_train)
y_pred_train = model.predict(X_train)
print('Train accuracy:',accuracy_score(y_train,y_pred_train))
y_pred_test = model.predict(X_test)
print('Test accuracy:',accuracy_score(y_test,y_pred_test))

model.fit(X,y)

"""## Save model"""

import joblib
import pickle

#Save pkl file
filename = 'model_svm.joblib'
joblib.dump(model, filename, compress=9)

